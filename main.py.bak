import camelot
import jieba
import streamlit as st
import tempfile
import os
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

class pdf2text:
    def __init__(self):
        pass

    def __call__(self, pdf_file):
        '''
        return list of text, a text corresponds to a table
        '''
        tables = camelot.read_pdf(pdf_file, pages='all')

        texts = []
        for table in tables:
            data = table.data

            data = map(lambda l: ' '.join(l), data)
            data = ' '.join(data)

            data = data.replace('\n', '')

            texts.append(data)
        return texts

    def get_table(self, pdf_file, index):
        '''
        return target table (pandas DataFrame)
        '''
        tables = camelot.read_pdf(pdf_file, pages='all')
        return tables[index].df

class text2vector:
    def __init__(self):
        self.vectorizer = TfidfVectorizer(tokenizer=lambda text: list(jieba.cut(text)))

    def fit_transform(self, texts):
        return self.vectorizer.fit_transform(texts)

    def transform(self, texts):
        return self.vectorizer.transform(texts)

class cosine_sim:
    def __init__(self):
        pass

    def __call__(self, table_vector, keyword_vector):
        return cosine_similarity(table_vector, keyword_vector)

def main(keyword, pdf_file):
    pdf_parser = pdf2text()
    table_texts = pdf_parser(pdf_file)

    keyword_vectorizer = text2vector()
    table_vectorizer = text2vector()

    all_texts = [keyword] + table_texts  # Combine keyword and table texts

    all_vectors = keyword_vectorizer.fit_transform(all_texts)
    
    keyword_vector = all_vectors[0]
    table_vectors = all_vectors[1:]

    similarity_calculator = cosine_sim()
    similarity = similarity_calculator(table_vectors, keyword_vector)

    if similarity.size > 0:
        max_sim_index = similarity.argmax()
        #max_sim_index = 8
        print("max_sim_index=", max_sim_index)
        #print("sim=", similarity[0, max_sim_index])
        if similarity[max_sim_index] > 0:
        	max_sim_table = pdf_parser.get_table(pdf_file, max_sim_index)
        #	print("Most similar table:\n", max_sim_table)
        #else:
        #    print("Keyword not found in the table.")
    #else:
    #    print("No tables found in the document.")
    return (max_sim_table)    
    

if __name__ == "__main__":
    st.title("Welcome to the keyword search system of Black Cat🐈‍ (B11902014 高浩鈞)")
    input_file = st.file_uploader("Please upload your PDF file: ", type = 'pdf')
    
    #if input_file is None: 
    if input_file:
    	temp_dir = tempfile.mkdtemp(dir='.', suffix='.pdf')
    	file_path = os.path.join(temp_dir, 'uploaded_file.pdf')
    	with open(file_path, "wb") as f:
    		f.write(input_file.getvalue())
    else:
        st.warning("Please upload a PDF file.")
        st.stop()
        
    keyword = st.text_input("Please enter a keyword: ")
    if not keyword:
        st.warning("Please enter a keyword.")
        st.stop()
    
    table = main(keyword, file_path)
    if table:
    	print("Most similar table: ")
    	st.table(table)
    else:
    	print("Keyword not found in the table.")
    #for each_file in input_file:
    #	print("Filename:", each_file.name)
    #	main(keyword, each_file.name)
	    #bytes_data = uploaded_file.read()
	    #st.write("filename:", uploaded_file.name)
	    #st.write(bytes_data)
	    
	    
    #keyword = '非監督式學習的應用'
    #keyword = '發現數據中隱藏的結構'
    #keyword = '多細胞生物細胞膜和'
    #keyword = 'abc'
    #main(keyword, "ai_tables_#1.pdf")
    #input_file = ai_tables_#1.pdf
    #main(keyword, "ai_tables_#2.pdf")
    
    #print(input_file)
    #print(keyword)